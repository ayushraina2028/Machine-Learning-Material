# %%
from keras.models import Sequential
from keras.layers import Dense
import matplotlib.pyplot as plt

# %%
import pyarrow.parquet as pq
import pandas as pd
pf = pd.read_parquet('data.parquet',dtype_backend = 'pyarrow')

# %%
df = pf[['X_jets','y']][:1000]

# %%
# Convert to numpy arrays
X = np.stack(df['X_jets'].values)
y = df['y'].values

# %%
print(X.shape, y.shape)

# %%
# Separate into training, validation, and test data

testLen = 0.2*len(X)
X_train = X[:int(len(X)-testLen)]
y_train = y[:int(len(X)-testLen)]

X_test = X[int(len(X)-testLen):]
y_test = y[int(len(X)-testLen):]

print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

# %%


# %%
# Separate into training and validation data
valLen = 0.2*len(X_train)
print(valLen)

X_valid = X_train[:int(valLen)]
y_valid = y_train[:int(valLen)]

print(X_valid.shape, y_valid.shape)

X_train = X_train[int(valLen):]
y_train = y_train[int(valLen):]

print(X_train.shape, y_train.shape)

# %%
print(X_train.shape, X_valid.shape, X_test.shape)
print(y_train.shape, y_valid.shape, y_test.shape)

# %%
# Take transpose of X data
X_train = np.transpose(X_train, (0,2,3,1))
X_test = np.transpose(X_test, (0,2,3,1))
X_valid = np.transpose(X_valid, (0,2,3,1))

print(X_train.shape, X_valid.shape, X_test.shape)

# %%
# Normalizing the data
mean = np.mean(X_train, axis=(0,1,2,3))
std = np.std(X_train, axis=(0,1,2,3))

# %%
X_train = (X_train - mean) / (std + 1e-7)
X_valid = (X_valid - mean) / (std + 1e-7)
X_test = (X_test - mean) / (std + 1e-7)

# %%
# Import necessary libraries
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D
from keras.callbacks import ModelCheckpoint
from keras import regularizers, optimizers


# %%
# Build Model Structure (Modified Version Of Alexnet)
base_hidden_units = 32
weight_decay = 1e-4
model = Sequential()

# %%


# %%
from skimage.transform import resize

# Resize all images to 32,32 in X_train
X_train = np.array([resize(image, (32,32)) for image in X_train])
X_valid = np.array([resize(image, (32,32)) for image in X_valid])
X_test = np.array([resize(image, (32,32)) for image in X_test])

print(X_train.shape, X_valid.shape, X_test.shape)

# %%
print(y_train.shape, y_valid.shape, y_test.shape)

# %%
from keras.utils import to_categorical
y_train = to_categorical(y_train, 2)
y_valid = to_categorical(y_valid, 2)
y_test = to_categorical(y_test, 2)

# %%


# %%
# Convolutional Layer 1
model.add(Conv2D(base_hidden_units, kernel_size=(3,3),padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=X_train.shape[1:]))
model.add(Activation('relu'))
model.add(BatchNormalization())

# Convolutional Layer 2
model.add(Conv2D(base_hidden_units,kernel_size=(3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())

# Pool + Dropout
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.2))

#Convolutional Layer 3
model.add(Conv2D(base_hidden_units*2, kernel_size=(3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())

#Convolutional Layer 4
model.add(Conv2D(base_hidden_units*2, kernel_size=(3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())

# Pool + Dropout
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.3))

#Convolutional Layer 5
model.add(Conv2D(base_hidden_units*4, kernel_size=(3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())

#Convolutional Layer 6
model.add(Conv2D(base_hidden_units*4, kernel_size=(3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())

# Pool + Dropout
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.4))

# Fully Connected Layer 1
model.add(Flatten())
model.add(Dense(2, activation='softmax'))

model.summary()


# %%
batch_size = 128
epochs = 50

#USe model checkpoint to save only the best model
checkpointer = ModelCheckpoint(filepath='model.weights.best.keras', verbose=1, save_best_only=True)

#adam optimizer
opt = keras.optimizers.Adam(learning_rate=0.001)

model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

# %%
# # Make data as tensor
# import tensorflow as tf
# X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)
# X_valid = tf.convert_to_tensor(X_valid, dtype=tf.float32)
# X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)

# %%
history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=[checkpointer], verbose=2, shuffle=True, steps_per_epoch=X_train.shape[0]//batch_size)

# %%



